{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99d7e6d-4746-4c51-b304-b329eb3ffddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPR6223 Pattern Recognition Group Project\n",
    "# Automobile Brand Recognition System\n",
    "# \n",
    "# Group: Byte Me\n",
    "# Lab Section: 1B\n",
    "# Group Leader: Do Wai Lung\n",
    "# Members: \n",
    "#   1. Doris Heng\n",
    "#   2. Eldeena Lim Huey Yinn  \n",
    "#   3. Kong Yi Xuan\n",
    "#\n",
    "# This project recognizes automobile brands from logo images using machine learning\n",
    "# We implemented all the techniques we learned in our labs this semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee59c4b-ce78-4961-92a2-10b815569dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Lab 8 method\n",
    "from sklearn.svm import SVC  # Lab 10 method\n",
    "from sklearn.neural_network import MLPClassifier  # Lab 11 method\n",
    "from sklearn.ensemble import VotingClassifier  # Extra credit\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Keep output clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ea5744-0488-481a-b4e2-4c960748525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logo_image(image_path, size):\n",
    "    \"\"\"\n",
    "    Load and preprocess an automobile logo image.\n",
    "    We had to figure out how to handle different lighting and image sizes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: path to image file\n",
    "        size: target size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        processed image or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image with OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Convert BGR to RGB (OpenCV uses BGR by default)\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to standard size\n",
    "        resized = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Improve contrast for different lighting conditions\n",
    "        # We use CLAHE which we learned about in image processing\n",
    "        if len(resized.shape) == 3:\n",
    "            # Convert to LAB color space\n",
    "            lab = cv2.cvtColor(resized, cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            \n",
    "            # Apply CLAHE to lightness channel\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "            l_enhanced = clahe.apply(l)\n",
    "            \n",
    "            # Combine back to RGB\n",
    "            lab_enhanced = cv2.merge((l_enhanced, a, b))\n",
    "            enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))\n",
    "            enhanced = clahe.apply(resized)\n",
    "        \n",
    "        # Convert to grayscale for our analysis\n",
    "        if len(enhanced.shape) == 3:\n",
    "            gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = enhanced\n",
    "        \n",
    "        # Small blur to reduce noise from phone cameras\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        normalized = blurred.astype(np.float32) / 255.0\n",
    "        \n",
    "        return normalized\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb1d4ca-b257-40d6-8e09-281d16a1cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_automobile_logos(folders, labels, img_size):\n",
    "    \"\"\"\n",
    "    Load all our collected automobile logo images.\n",
    "    We manually collected and cropped these photos as required.\n",
    "    \n",
    "    Args:\n",
    "        folders: dict of brand -> folder path\n",
    "        labels: dict of brand -> numeric label\n",
    "        img_size: target image size\n",
    "    \n",
    "    Returns:\n",
    "        images array, labels array, file paths\n",
    "    \"\"\"\n",
    "    print(\"Loading our automobile logo dataset...\")\n",
    "    print(\"These are photos we collected ourselves around Melaka\")\n",
    "    \n",
    "    images = []\n",
    "    image_labels = []\n",
    "    file_paths = []\n",
    "    \n",
    "    # Load each brand's images\n",
    "    for brand, folder in folders.items():\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"Warning: {folder} not found for {brand}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nLoading {brand.upper()} logos from {folder}...\")\n",
    "        \n",
    "        # Get all image files\n",
    "        image_files = [f for f in os.listdir(folder) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        \n",
    "        print(f\"Found {len(image_files)} {brand} images\")\n",
    "        \n",
    "        # Process each image\n",
    "        loaded_count = 0\n",
    "        for img_file in image_files:\n",
    "            full_path = os.path.join(folder, img_file)\n",
    "            processed = preprocess_logo_image(full_path, img_size)\n",
    "            \n",
    "            if processed is not None:\n",
    "                images.append(processed)\n",
    "                image_labels.append(labels[brand])\n",
    "                file_paths.append(full_path)\n",
    "                loaded_count += 1\n",
    "        \n",
    "        print(f\"Successfully loaded {loaded_count} {brand} images\")\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(\"ERROR: No images loaded!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    images_array = np.array(images)\n",
    "    labels_array = np.array(image_labels)\n",
    "    \n",
    "    print(f\"\\nDataset summary:\")\n",
    "    print(f\"Total images: {len(images_array)}\")\n",
    "    print(f\"Image shape: {images_array.shape}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
    "    brand_names = {v: k for k, v in labels.items()}\n",
    "    print(\"\\nBrand distribution:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"  {brand_names[label].upper()}: {count} images\")\n",
    "    \n",
    "    return images_array, labels_array, file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7eabc8-a29c-447a-99a0-6a1ee06f64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset_with_augmentation(images, labels):\n",
    "    \"\"\"\n",
    "    Balance our dataset by adding augmented images.\n",
    "    This helps prevent bias toward brands with more photos.\n",
    "    \n",
    "    Args:\n",
    "        images: original images\n",
    "        labels: corresponding labels\n",
    "    \n",
    "    Returns:\n",
    "        balanced images and labels\n",
    "    \"\"\"\n",
    "    print(\"\\nBalancing dataset with augmentation...\")\n",
    "    \n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    brand_names = {0: 'Toyota', 1: 'Honda', 2: 'Mazda', 3: 'Perodua'}\n",
    "    \n",
    "    print(\"Original distribution:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"  {brand_names[label]}: {count} images\")\n",
    "    \n",
    "    # Set target number per brand\n",
    "    max_count = max(counts)\n",
    "    target = min(max_count + 15, 160)  # Not too many to avoid overfitting\n",
    "    \n",
    "    print(f\"Target per brand: {target} images\")\n",
    "    \n",
    "    balanced_images = []\n",
    "    balanced_labels = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Get images for this brand\n",
    "        brand_mask = labels == label\n",
    "        brand_images = images[brand_mask]\n",
    "        current_count = len(brand_images)\n",
    "        \n",
    "        print(f\"\\nProcessing {brand_names[label]}...\")\n",
    "        \n",
    "        # Add all original images\n",
    "        for img in brand_images:\n",
    "            balanced_images.append(img)\n",
    "            balanced_labels.append(label)\n",
    "        \n",
    "        # Add augmented images if needed\n",
    "        if current_count < target:\n",
    "            needed = target - current_count\n",
    "            print(f\"  Adding {needed} augmented images\")\n",
    "            \n",
    "            for i in range(needed):\n",
    "                # Pick random original image\n",
    "                base_img = brand_images[i % current_count]\n",
    "                \n",
    "                # Create augmented version\n",
    "                augmented = simple_augment(base_img)\n",
    "                if augmented is not None:\n",
    "                    balanced_images.append(augmented)\n",
    "                    balanced_labels.append(label)\n",
    "        \n",
    "        final_count = len([l for l in balanced_labels if l == label])\n",
    "        print(f\"  Final {brand_names[label]} count: {final_count}\")\n",
    "    \n",
    "    return np.array(balanced_images), np.array(balanced_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6529d51-b2ed-414f-8caf-ba5eb7a2e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_augment(image):\n",
    "    \"\"\"\n",
    "    Apply simple augmentation to an image.\n",
    "    We keep changes small to preserve logo recognition.\n",
    "    \n",
    "    Args:\n",
    "        image: input image\n",
    "    \n",
    "    Returns:\n",
    "        augmented image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        h, w = image.shape\n",
    "        \n",
    "        # Small rotation (-5 to +5 degrees)\n",
    "        angle = np.random.uniform(-5, 5)\n",
    "        center = (w//2, h//2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        rotated = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "        \n",
    "        # Slight brightness change (0.85 to 1.15 times original)\n",
    "        brightness = np.random.uniform(0.85, 1.15)\n",
    "        adjusted = np.clip(rotated * brightness, 0, 1)\n",
    "        \n",
    "        # Small scaling (0.95 to 1.05 times original)\n",
    "        scale = np.random.uniform(0.95, 1.05)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        scaled = cv2.resize(adjusted, (new_w, new_h))\n",
    "        \n",
    "        # Crop or pad back to original size\n",
    "        if new_h >= h and new_w >= w:\n",
    "            # Crop from center\n",
    "            start_y = (new_h - h) // 2\n",
    "            start_x = (new_w - w) // 2\n",
    "            final = scaled[start_y:start_y+h, start_x:start_x+w]\n",
    "        else:\n",
    "            # Pad to original size\n",
    "            pad_y = max(0, (h - new_h) // 2)\n",
    "            pad_x = max(0, (w - new_w) // 2)\n",
    "            final = np.pad(scaled, ((pad_y, h-new_h-pad_y), (pad_x, w-new_w-pad_x)), mode='edge')\n",
    "            if final.shape != (h, w):\n",
    "                final = cv2.resize(final, (w, h))\n",
    "        \n",
    "        return final.astype(np.float32)\n",
    "        \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75fb7a81-9269-4a75-9ddd-2841869ae441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_pca_lda(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Extract features using PCA and LDA methods from our labs.\n",
    "    \n",
    "    Args:\n",
    "        X_train: training data (flattened images)\n",
    "        X_test: test data (flattened images)\n",
    "        y_train: training labels\n",
    "    \n",
    "    Returns:\n",
    "        dictionary with all feature sets and models\n",
    "    \"\"\"\n",
    "    print(\"Extracting features using PCA and LDA...\")\n",
    "    \n",
    "    # Step 1: Standardize the data\n",
    "    print(\"  Standardizing data...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Step 2: Apply PCA (Lab 6 method)\n",
    "    print(\"  Applying PCA (Lab 6)...\")\n",
    "    \n",
    "    # Find number of components for 92% variance\n",
    "    pca_full = PCA(random_state=42)\n",
    "    pca_full.fit(X_train_scaled)\n",
    "    \n",
    "    cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumvar >= 0.92) + 1\n",
    "    \n",
    "    print(f\"    Using {n_components} components for 92% variance\")\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components, whiten=True, random_state=42)\n",
    "    pca.fit(X_train_scaled)\n",
    "    \n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    print(f\"    PCA: {X_train_scaled.shape[1]} → {n_components} features\")\n",
    "    \n",
    "    # Step 3: Apply LDA (Lab 7 method)\n",
    "    print(\"  Applying LDA (Lab 7)...\")\n",
    "    \n",
    "    # LDA can create max (n_classes - 1) features\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    lda_components = n_classes - 1  # 3 for our 4 brands\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(n_components=lda_components, solver='svd')\n",
    "    lda.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    X_train_lda = lda.transform(X_train_scaled)\n",
    "    X_test_lda = lda.transform(X_test_scaled)\n",
    "    \n",
    "    print(f\"    LDA: {X_train_scaled.shape[1]} → {lda_components} features\")\n",
    "    \n",
    "    return {\n",
    "        'scaler': scaler,\n",
    "        'pca': pca,\n",
    "        'lda': lda,\n",
    "        'X_train_pca': X_train_pca,\n",
    "        'X_test_pca': X_test_pca,\n",
    "        'X_train_lda': X_train_lda,\n",
    "        'X_test_lda': X_test_lda,\n",
    "        'n_pca_components': n_components\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d22c5a8-7335-4007-afbf-a57367ad477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(features, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train all classification models from our labs.\n",
    "    \n",
    "    Args:\n",
    "        features: extracted features dictionary\n",
    "        y_train: training labels\n",
    "        y_test: test labels\n",
    "    \n",
    "    Returns:\n",
    "        dictionary with all model results\n",
    "    \"\"\"\n",
    "    print(\"Training all classification models...\")\n",
    "    \n",
    "    # Get feature sets\n",
    "    X_train_pca = features['X_train_pca']\n",
    "    X_test_pca = features['X_test_pca']\n",
    "    X_train_lda = features['X_train_lda']\n",
    "    X_test_lda = features['X_test_lda']\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # k-NN models (Lab 8)\n",
    "    print(\"\\n  Training k-NN models (Lab 8)...\")\n",
    "    \n",
    "    # k-NN with PCA\n",
    "    knn_pca = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "    knn_pca.fit(X_train_pca, y_train)\n",
    "    knn_pca_pred = knn_pca.predict(X_test_pca)\n",
    "    knn_pca_cv = cross_val_score(knn_pca, X_train_pca, y_train, cv=cv)\n",
    "    \n",
    "    results['knn_pca'] = {\n",
    "        'model': knn_pca,\n",
    "        'feature_type': 'PCA',\n",
    "        'test_accuracy': accuracy_score(y_test, knn_pca_pred),\n",
    "        'cv_mean': knn_pca_cv.mean(),\n",
    "        'cv_std': knn_pca_cv.std(),\n",
    "        'predictions': knn_pca_pred\n",
    "    }\n",
    "    print(f\"    k-NN + PCA: {results['knn_pca']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # k-NN with LDA\n",
    "    knn_lda = KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "    knn_lda.fit(X_train_lda, y_train)\n",
    "    knn_lda_pred = knn_lda.predict(X_test_lda)\n",
    "    knn_lda_cv = cross_val_score(knn_lda, X_train_lda, y_train, cv=cv)\n",
    "    \n",
    "    results['knn_lda'] = {\n",
    "        'model': knn_lda,\n",
    "        'feature_type': 'LDA',\n",
    "        'test_accuracy': accuracy_score(y_test, knn_lda_pred),\n",
    "        'cv_mean': knn_lda_cv.mean(),\n",
    "        'cv_std': knn_lda_cv.std(),\n",
    "        'predictions': knn_lda_pred\n",
    "    }\n",
    "    print(f\"    k-NN + LDA: {results['knn_lda']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # SVM models (Lab 10)\n",
    "    print(\"\\n  Training SVM models (Lab 10)...\")\n",
    "    \n",
    "    # Linear SVM with PCA\n",
    "    svm_linear_pca = SVC(kernel='linear', C=5.0, random_state=42)\n",
    "    svm_linear_pca.fit(X_train_pca, y_train)\n",
    "    svm_linear_pca_pred = svm_linear_pca.predict(X_test_pca)\n",
    "    svm_linear_pca_cv = cross_val_score(svm_linear_pca, X_train_pca, y_train, cv=cv)\n",
    "    \n",
    "    results['svm_linear_pca'] = {\n",
    "        'model': svm_linear_pca,\n",
    "        'feature_type': 'PCA',\n",
    "        'test_accuracy': accuracy_score(y_test, svm_linear_pca_pred),\n",
    "        'cv_mean': svm_linear_pca_cv.mean(),\n",
    "        'cv_std': svm_linear_pca_cv.std(),\n",
    "        'predictions': svm_linear_pca_pred\n",
    "    }\n",
    "    print(f\"    Linear SVM + PCA: {results['svm_linear_pca']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # RBF SVM with PCA\n",
    "    svm_rbf_pca = SVC(kernel='rbf', C=100, gamma='scale', random_state=42)\n",
    "    svm_rbf_pca.fit(X_train_pca, y_train)\n",
    "    svm_rbf_pca_pred = svm_rbf_pca.predict(X_test_pca)\n",
    "    svm_rbf_pca_cv = cross_val_score(svm_rbf_pca, X_train_pca, y_train, cv=cv)\n",
    "    \n",
    "    results['svm_rbf_pca'] = {\n",
    "        'model': svm_rbf_pca,\n",
    "        'feature_type': 'PCA',\n",
    "        'test_accuracy': accuracy_score(y_test, svm_rbf_pca_pred),\n",
    "        'cv_mean': svm_rbf_pca_cv.mean(),\n",
    "        'cv_std': svm_rbf_pca_cv.std(),\n",
    "        'predictions': svm_rbf_pca_pred\n",
    "    }\n",
    "    print(f\"    RBF SVM + PCA: {results['svm_rbf_pca']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # Linear SVM with LDA\n",
    "    svm_linear_lda = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    svm_linear_lda.fit(X_train_lda, y_train)\n",
    "    svm_linear_lda_pred = svm_linear_lda.predict(X_test_lda)\n",
    "    svm_linear_lda_cv = cross_val_score(svm_linear_lda, X_train_lda, y_train, cv=cv)\n",
    "    \n",
    "    results['svm_linear_lda'] = {\n",
    "        'model': svm_linear_lda,\n",
    "        'feature_type': 'LDA',\n",
    "        'test_accuracy': accuracy_score(y_test, svm_linear_lda_pred),\n",
    "        'cv_mean': svm_linear_lda_cv.mean(),\n",
    "        'cv_std': svm_linear_lda_cv.std(),\n",
    "        'predictions': svm_linear_lda_pred\n",
    "    }\n",
    "    print(f\"    Linear SVM + LDA: {results['svm_linear_lda']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # Neural Network models (Lab 11)\n",
    "    print(\"\\n  Training Neural Networks (Lab 11)...\")\n",
    "    \n",
    "    # MLP with PCA\n",
    "    mlp_pca = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.01,\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp_pca.fit(X_train_pca, y_train)\n",
    "    mlp_pca_pred = mlp_pca.predict(X_test_pca)\n",
    "    mlp_pca_cv = cross_val_score(mlp_pca, X_train_pca, y_train, cv=cv)\n",
    "    \n",
    "    results['mlp_pca'] = {\n",
    "        'model': mlp_pca,\n",
    "        'feature_type': 'PCA',\n",
    "        'test_accuracy': accuracy_score(y_test, mlp_pca_pred),\n",
    "        'cv_mean': mlp_pca_cv.mean(),\n",
    "        'cv_std': mlp_pca_cv.std(),\n",
    "        'predictions': mlp_pca_pred\n",
    "    }\n",
    "    print(f\"    Neural Network + PCA: {results['mlp_pca']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # MLP with LDA\n",
    "    mlp_lda = MLPClassifier(\n",
    "        hidden_layer_sizes=(20, 10),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.01,\n",
    "        max_iter=300,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp_lda.fit(X_train_lda, y_train)\n",
    "    mlp_lda_pred = mlp_lda.predict(X_test_lda)\n",
    "    mlp_lda_cv = cross_val_score(mlp_lda, X_train_lda, y_train, cv=cv)\n",
    "    \n",
    "    results['mlp_lda'] = {\n",
    "        'model': mlp_lda,\n",
    "        'feature_type': 'LDA',\n",
    "        'test_accuracy': accuracy_score(y_test, mlp_lda_pred),\n",
    "        'cv_mean': mlp_lda_cv.mean(),\n",
    "        'cv_std': mlp_lda_cv.std(),\n",
    "        'predictions': mlp_lda_pred\n",
    "    }\n",
    "    print(f\"    Neural Network + LDA: {results['mlp_lda']['test_accuracy']:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e9a680-2137-4b31-a906-00e0210d31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble(model_results, features, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Create ensemble from best models (extra credit attempt).\n",
    "    \n",
    "    Args:\n",
    "        model_results: individual model results\n",
    "        features: feature data\n",
    "        y_train: training labels\n",
    "        y_test: test labels\n",
    "    \n",
    "    Returns:\n",
    "        ensemble results dictionary\n",
    "    \"\"\"\n",
    "    print(\"\\n  Creating ensemble (extra credit)...\")\n",
    "    \n",
    "    # Select models with reasonable performance\n",
    "    good_models = []\n",
    "    \n",
    "    for name, result in model_results.items():\n",
    "        cv_score = result['cv_mean']\n",
    "        test_score = result['test_accuracy']\n",
    "        gap = abs(test_score - cv_score)\n",
    "        \n",
    "        # Include if decent performance and not too much overfitting\n",
    "        if cv_score > 0.3 and test_score > 0.3 and gap < 0.3:\n",
    "            good_models.append((name, result))\n",
    "    \n",
    "    if len(good_models) >= 2:\n",
    "        # Use top 3 models for ensemble\n",
    "        ensemble_models = [(name, result['model']) for name, result in good_models[:3]]\n",
    "        ensemble = VotingClassifier(estimators=ensemble_models, voting='hard')\n",
    "        \n",
    "        # Train on PCA features\n",
    "        X_train_pca = features['X_train_pca']\n",
    "        X_test_pca = features['X_test_pca']\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        ensemble_cv = cross_val_score(ensemble, X_train_pca, y_train, cv=cv)\n",
    "        \n",
    "        ensemble.fit(X_train_pca, y_train)\n",
    "        ensemble_pred = ensemble.predict(X_test_pca)\n",
    "        ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n",
    "        \n",
    "        print(f\"    Ensemble: {ensemble_accuracy:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'ensemble': {\n",
    "                'model': ensemble,\n",
    "                'feature_type': 'PCA',\n",
    "                'test_accuracy': ensemble_accuracy,\n",
    "                'cv_mean': ensemble_cv.mean(),\n",
    "                'cv_std': ensemble_cv.std(),\n",
    "                'predictions': ensemble_pred\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        print(\"    Not enough good models for ensemble\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c40c2fc-8827-4f09-9b25-b33d62f84b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(all_results):\n",
    "    \"\"\"\n",
    "    Analyze and display all results clearly.\n",
    "    \n",
    "    Args:\n",
    "        all_results: dictionary of all model results\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with organized results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Organize results\n",
    "    results_data = []\n",
    "    for model_name, result in all_results.items():\n",
    "        if 'cv_mean' in result:\n",
    "            # Calculate overfitting gap\n",
    "            gap = abs(result['test_accuracy'] - result['cv_mean'])\n",
    "            \n",
    "            # Determine generalization quality\n",
    "            if gap < 0.1:\n",
    "                quality = 'Excellent'\n",
    "            elif gap < 0.2:\n",
    "                quality = 'Good'\n",
    "            else:\n",
    "                quality = 'Poor'\n",
    "            \n",
    "            results_data.append({\n",
    "                'Model': model_name,\n",
    "                'Features': result['feature_type'],\n",
    "                'CV_Score': result['cv_mean'],\n",
    "                'Test_Score': result['test_accuracy'],\n",
    "                'Gap': gap,\n",
    "                'Generalization': quality\n",
    "            })\n",
    "    \n",
    "    # Create and sort results table\n",
    "    df = pd.DataFrame(results_data)\n",
    "    df = df.sort_values('Test_Score', ascending=False)\n",
    "    \n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"{row['Model']:15} | {row['Features']:3} | \"\n",
    "              f\"CV: {row['CV_Score']:.3f} | Test: {row['Test_Score']:.3f} | \"\n",
    "              f\"Gap: {row['Gap']:.3f} | {row['Generalization']}\")\n",
    "    \n",
    "    # Find best model\n",
    "    good_models = df[df['Generalization'].isin(['Excellent', 'Good'])]\n",
    "    if len(good_models) > 0:\n",
    "        best = good_models.iloc[0]\n",
    "        print(f\"\\n BEST MODEL: {best['Model']}\")\n",
    "        print(f\"   Test Accuracy: {best['Test_Score']:.3f} ({best['Test_Score']*100:.1f}%)\")\n",
    "        print(f\"   CV Score: {best['CV_Score']:.3f}\")\n",
    "        print(f\"   Generalization: {best['Generalization']}\")\n",
    "        \n",
    "        # Performance interpretation\n",
    "        if best['Test_Score'] >= 0.7:\n",
    "            print(\"   Excellent performance for logo recognition.\")\n",
    "        elif best['Test_Score'] >= 0.5:\n",
    "            print(\"   Good performance\")\n",
    "        else:\n",
    "            print(\"   Decent performance\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f32b2952-3e93-4361-8564-99c7506d1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(all_results, y_test):\n",
    "    \"\"\"\n",
    "    Show confusion matrix for best model.\n",
    "    \n",
    "    Args:\n",
    "        all_results: all model results\n",
    "        y_test: true test labels\n",
    "    \"\"\"\n",
    "    # Find best model\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for name, result in all_results.items():\n",
    "        if 'test_accuracy' in result:\n",
    "            gap = abs(result['test_accuracy'] - result['cv_mean'])\n",
    "            if result['test_accuracy'] > best_score and gap < 0.25:\n",
    "                best_score = result['test_accuracy']\n",
    "                best_model = name\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nConfusion Matrix - {best_model}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        predictions = all_results[best_model]['predictions']\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        \n",
    "        brands = ['Toyota', 'Honda', 'Mazda', 'Perodua']\n",
    "        \n",
    "        # Print confusion matrix\n",
    "        print(\"        Predicted:\")\n",
    "        print(\"        \", \"  \".join(f\"{b[:6]:>6}\" for b in brands))\n",
    "        print(\"Actual:\")\n",
    "        for i, actual_brand in enumerate(brands):\n",
    "            row = f\"{actual_brand[:6]:>6}: \"\n",
    "            for j in range(len(brands)):\n",
    "                row += f\"{cm[i][j]:>6}  \"\n",
    "            print(row)\n",
    "        \n",
    "        # Calculate per-brand accuracy\n",
    "        print(\"\\nPer-brand accuracy:\")\n",
    "        for i, brand in enumerate(brands):\n",
    "            if cm[i].sum() > 0:\n",
    "                acc = cm[i][i] / cm[i].sum()\n",
    "                print(f\"  {brand}: {acc:.3f} ({acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1de45aa7-7e6d-434c-a205-fd0776bc2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_project():\n",
    "    \"\"\"\n",
    "    Main function that runs our automobile brand recognition project.\n",
    "    This implements all requirements and follows our lab exercises.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"AUTOMOBILE BRAND RECOGNITION SYSTEM\")\n",
    "    print(\"TPR6223 Pattern Recognition Group Project\")\n",
    "    print(\"\")\n",
    "    print(\"Group: Byte Me (Lab Section 1B)\")\n",
    "    print(\"Leader: Do Wai Lung\")\n",
    "    print(\"Members: Doris Heng, Eldeena Lim Huey Yinn, Kong Yi Xuan\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Project settings\n",
    "    image_size = (128, 128)\n",
    "    \n",
    "    # Brand mapping\n",
    "    brands = {\n",
    "        'toyota': 0,\n",
    "        'honda': 1, \n",
    "        'mazda': 2,\n",
    "        'perodua': 3\n",
    "    }\n",
    "    \n",
    "    # Our data folders\n",
    "    folders = {\n",
    "        'toyota': \"Project/Manual_Cropped_Logo/Toyota\",\n",
    "        'honda': \"Project/Manual_Cropped_Logo/Honda\",\n",
    "        'mazda': \"Project/Manual_Cropped_Logo/Mazda\", \n",
    "        'perodua': \"Project/Manual_Cropped_Logo/Perodua\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSTEP 1: Loading automobile logo dataset\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Load our collected images\n",
    "    images, labels, file_paths = load_automobile_logos(folders, brands, image_size)\n",
    "    \n",
    "    if images is None:\n",
    "        print(\"ERROR: Failed to load dataset!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nSTEP 2: Balancing dataset\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Balance dataset with augmentation\n",
    "    balanced_images, balanced_labels = balance_dataset_with_augmentation(images, labels)\n",
    "    \n",
    "    print(\"\\nSTEP 3: Train-test split (60/40)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Flatten images for sklearn\n",
    "    num_pixels = image_size[0] * image_size[1]\n",
    "    X_flat = balanced_images.reshape(len(balanced_images), num_pixels)\n",
    "    \n",
    "    # Split data as required (60% train, 40% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_flat, balanced_labels,\n",
    "        test_size=0.4,\n",
    "        random_state=42,\n",
    "        stratify=balanced_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} images\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} images\")\n",
    "    \n",
    "    # Show distribution\n",
    "    train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
    "    brand_names = ['Toyota', 'Honda', 'Mazda', 'Perodua']\n",
    "    \n",
    "    print(\"\\nTrain distribution:\")\n",
    "    for label, count in zip(train_unique, train_counts):\n",
    "        print(f\"  {brand_names[label]}: {count}\")\n",
    "    \n",
    "    print(\"Test distribution:\")\n",
    "    for label, count in zip(test_unique, test_counts):\n",
    "        print(f\"  {brand_names[label]}: {count}\")\n",
    "    \n",
    "    print(\"\\nSTEP 4: Feature extraction\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Extract features using PCA and LDA\n",
    "    features = extract_features_pca_lda(X_train, X_test, y_train)\n",
    "    \n",
    "    print(\"\\nSTEP 5: Training classifiers\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train all required models\n",
    "    model_results = train_all_models(features, y_train, y_test)\n",
    "    \n",
    "    print(\"\\nSTEP 6: Creating ensemble\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Try ensemble approach\n",
    "    ensemble_results = create_ensemble(model_results, features, y_train, y_test)\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = {**model_results, **ensemble_results}\n",
    "    \n",
    "    print(\"\\nSTEP 7: Results analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Analyze all results\n",
    "    results_df = analyze_results(all_results)\n",
    "    \n",
    "    return {\n",
    "        'results': all_results,\n",
    "        'features': features,\n",
    "        'results_df': results_df,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d5f00a9-28c3-42a3-91da-cee09246f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Group Byte Me's automobile recognition project...\n",
      "Make sure image folders are set up correctly!\n",
      "================================================================================\n",
      "AUTOMOBILE BRAND RECOGNITION SYSTEM\n",
      "TPR6223 Pattern Recognition Group Project\n",
      "\n",
      "Group: Byte Me (Lab Section 1B)\n",
      "Leader: Do Wai Lung\n",
      "Members: Doris Heng, Eldeena Lim Huey Yinn, Kong Yi Xuan\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Loading automobile logo dataset\n",
      "--------------------------------------------------\n",
      "Loading our automobile logo dataset...\n",
      "These are photos we collected ourselves around Melaka\n",
      "\n",
      "Loading TOYOTA logos from Project/Manual_Cropped_Logo/Toyota...\n",
      "Found 145 toyota images\n",
      "Successfully loaded 145 toyota images\n",
      "\n",
      "Loading HONDA logos from Project/Manual_Cropped_Logo/Honda...\n",
      "Found 156 honda images\n",
      "Successfully loaded 156 honda images\n",
      "\n",
      "Loading MAZDA logos from Project/Manual_Cropped_Logo/Mazda...\n",
      "Found 130 mazda images\n",
      "Successfully loaded 130 mazda images\n",
      "\n",
      "Loading PERODUA logos from Project/Manual_Cropped_Logo/Perodua...\n",
      "Found 100 perodua images\n",
      "Successfully loaded 100 perodua images\n",
      "\n",
      "Dataset summary:\n",
      "Total images: 531\n",
      "Image shape: (531, 128, 128)\n",
      "\n",
      "Brand distribution:\n",
      "  TOYOTA: 145 images\n",
      "  HONDA: 156 images\n",
      "  MAZDA: 130 images\n",
      "  PERODUA: 100 images\n",
      "\n",
      "STEP 2: Balancing dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "Balancing dataset with augmentation...\n",
      "Original distribution:\n",
      "  Toyota: 145 images\n",
      "  Honda: 156 images\n",
      "  Mazda: 130 images\n",
      "  Perodua: 100 images\n",
      "Target per brand: 160 images\n",
      "\n",
      "Processing Toyota...\n",
      "  Adding 15 augmented images\n",
      "  Final Toyota count: 160\n",
      "\n",
      "Processing Honda...\n",
      "  Adding 4 augmented images\n",
      "  Final Honda count: 160\n",
      "\n",
      "Processing Mazda...\n",
      "  Adding 30 augmented images\n",
      "  Final Mazda count: 160\n",
      "\n",
      "Processing Perodua...\n",
      "  Adding 60 augmented images\n",
      "  Final Perodua count: 160\n",
      "\n",
      "STEP 3: Train-test split (60/40)\n",
      "--------------------------------------------------\n",
      "Training set: 384 images\n",
      "Testing set: 256 images\n",
      "\n",
      "Train distribution:\n",
      "  Toyota: 96\n",
      "  Honda: 96\n",
      "  Mazda: 96\n",
      "  Perodua: 96\n",
      "Test distribution:\n",
      "  Toyota: 64\n",
      "  Honda: 64\n",
      "  Mazda: 64\n",
      "  Perodua: 64\n",
      "\n",
      "STEP 4: Feature extraction\n",
      "--------------------------------------------------\n",
      "Extracting features using PCA and LDA...\n",
      "  Standardizing data...\n",
      "  Applying PCA (Lab 6)...\n",
      "    Using 119 components for 92% variance\n",
      "    PCA: 16384 → 119 features\n",
      "  Applying LDA (Lab 7)...\n",
      "    LDA: 16384 → 3 features\n",
      "\n",
      "STEP 5: Training classifiers\n",
      "--------------------------------------------------\n",
      "Training all classification models...\n",
      "\n",
      "  Training k-NN models (Lab 8)...\n",
      "    k-NN + PCA: 0.473\n",
      "    k-NN + LDA: 0.445\n",
      "\n",
      "  Training SVM models (Lab 10)...\n",
      "    Linear SVM + PCA: 0.480\n",
      "    RBF SVM + PCA: 0.664\n",
      "    Linear SVM + LDA: 0.457\n",
      "\n",
      "  Training Neural Networks (Lab 11)...\n",
      "    Neural Network + PCA: 0.398\n",
      "    Neural Network + LDA: 0.387\n",
      "\n",
      "STEP 6: Creating ensemble\n",
      "--------------------------------------------------\n",
      "\n",
      "  Creating ensemble (extra credit)...\n",
      "    Ensemble: 0.598\n",
      "\n",
      "STEP 7: Results analysis\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS ANALYSIS\n",
      "============================================================\n",
      "Model Performance Summary:\n",
      "------------------------------------------------------------\n",
      "svm_rbf_pca     | PCA | CV: 0.531 | Test: 0.664 | Gap: 0.133 | Good\n",
      "ensemble        | PCA | CV: 0.469 | Test: 0.598 | Gap: 0.129 | Good\n",
      "svm_linear_pca  | PCA | CV: 0.393 | Test: 0.480 | Gap: 0.087 | Excellent\n",
      "knn_pca         | PCA | CV: 0.383 | Test: 0.473 | Gap: 0.090 | Excellent\n",
      "svm_linear_lda  | LDA | CV: 0.922 | Test: 0.457 | Gap: 0.465 | Poor\n",
      "knn_lda         | LDA | CV: 0.922 | Test: 0.445 | Gap: 0.477 | Poor\n",
      "mlp_pca         | PCA | CV: 0.364 | Test: 0.398 | Gap: 0.034 | Excellent\n",
      "mlp_lda         | LDA | CV: 0.800 | Test: 0.387 | Gap: 0.413 | Poor\n",
      "\n",
      " BEST MODEL: svm_rbf_pca\n",
      "   Test Accuracy: 0.664 (66.4%)\n",
      "   CV Score: 0.531\n",
      "   Generalization: Good\n",
      "   Good performance\n"
     ]
    }
   ],
   "source": [
    "# Run the project\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Group Byte Me's automobile recognition project...\")\n",
    "    print(\"Make sure image folders are set up correctly!\")\n",
    "    \n",
    "    # Execute main project\n",
    "    project_data = main_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d3655ac-2228-43ed-87c2-84cb505c4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trained models to saved_models/...\n",
      "✅ All models saved successfully!\n",
      "Best model: svm_rbf_pca (66.4% accuracy)\n",
      "Models saved!\n"
     ]
    }
   ],
   "source": [
    "import automobile_code as core\n",
    "\n",
    "if 'project_data' in locals():\n",
    "    core.save_trained_models(project_data)\n",
    "    print(\"Models saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
